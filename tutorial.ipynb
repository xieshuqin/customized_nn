{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "This tutorial will walk you through the key features and their usage of our costomized DataParallel module. \n",
    "The code is tested on pytorch version 0.4.0 and python 3.6. Should work on other pytorch versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import nn as mynn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin by defining some network classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple networks\n",
    "class simple_networkA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(5, 2)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "class simple_networkB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "# complex network\n",
    "class complex_networkA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(5, 2)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        \n",
    "        return_dict = {}\n",
    "        return_dict['outputs'] = {}\n",
    "        return_dict['outputs']['out1'] = output\n",
    "        return_dict['outputs']['out2'] = output\n",
    "        return_dict['second_output'] = output\n",
    "        return return_dict\n",
    "    \n",
    "class complex_networkB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case one\n",
    "Here we want use a network to extract features, and another network to use these features to make prediction.\n",
    "We show how to avoid redundant scattering and gathering of the features in this case. \n",
    "\n",
    "By setting keyword `gather_to_one_device=False`, we avoid gathering all outputs to one single devices and return a tuple containing all the outputs. The tuple is safe because we have sorted the outputs according to their GPU device ids.\n",
    "\n",
    "By setting keyword `inputs_are_scattered=True`, we assume the inputs are already scattered into all GPUS and thus skip the scattering step. The inputs must be sorted according to GPU device ids, matching the order of outputs when `gather_to_one_device=False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestCaseOne(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.netA = simple_networkA()\n",
    "        self.netB = simple_networkB()\n",
    "        \n",
    "        self.optimizer_A = optim.Adam(self.netA.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        self.optimizer_B = optim.Adam(self.netB.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        \n",
    "    def convert_data_parallel(self):\n",
    "        self.netA = mynn.DataParallel(self.netA.cuda(), gather_to_one_device=False) # don't gather to one device\n",
    "        self.netB = mynn.DataParallel(self.netB.cuda(), inputs_are_scattered=True) # assume inputs are scattered\n",
    "        \n",
    "    def forward(self, input):\n",
    "        A_out = self.netA(input)\n",
    "        B_out = self.netB(A_out)\n",
    "        \n",
    "        loss = B_out.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer_A.step()\n",
    "        \n",
    "        # Try print out A's type\n",
    "        print('type of A_out is ', type(A_out)) # now it's a tuple\n",
    "        print('size of A_out content is ', A_out[0].size()) # should be (16 / num_gpus, 2)\n",
    "        \n",
    "        return B_out\n",
    "        \n",
    "test_model = TestCaseOne()\n",
    "test_model.convert_data_parallel()\n",
    "\n",
    "input = torch.ones(16, 5).cuda()\n",
    "output = test_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case two\n",
    "Now consider a more complicated case. Suppose a network not only extracts features but also performs some kind of prediction. Since the prediction won't be used by any other network, we wish to gather them. But for the features, we still want to keep them seperate. In other words, we only want some of the outputs be seperate and gather the rest. \n",
    "\n",
    "This can be done by assigning `except_keywords=[...]`. See the below example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestCaseTwo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.netA = complex_networkA()\n",
    "        self.netB = complex_networkB()\n",
    "        \n",
    "        self.optimizer_A = optim.Adam(self.netA.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        self.optimizer_B = optim.Adam(self.netB.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        \n",
    "    def convert_data_parallel(self):\n",
    "        self.netA = mynn.DataParallel(self.netA.cuda(), except_keywords=['out1', 'out2']) # gather all except out1,out2\n",
    "        self.netB = mynn.DataParallel(self.netB.cuda(), inputs_are_scattered=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        A_out = self.netA(input)\n",
    "        \n",
    "        # let's see the outputs of the except_keywords\n",
    "        out1 = A_out['outputs']['out1']\n",
    "        out2 = A_out['outputs']['out2']\n",
    "        print('out1 type is {}; out2 type is {}'.format(type(out1), type(out2)))\n",
    "        print('out1 element size is ', out1[0].size())\n",
    "        \n",
    "        # check out the other outputs\n",
    "        second_output = A_out['second_output']\n",
    "        print('second output type is {}'.format(second_output.type()))\n",
    "        print('second output size is ', second_output.size())\n",
    "        \n",
    "        # manipulating with the tuple outputs\n",
    "        B_in = tuple(o1 + o2 for o1, o2 in zip(out1, out2)) # time cost is very small.\n",
    "        \n",
    "        B_out = self.netB(B_in)\n",
    "        return B_out\n",
    "\n",
    "test_model = TestCaseTwo()\n",
    "test_model.convert_data_parallel()\n",
    "\n",
    "input = torch.ones(16, 5).cuda()\n",
    "output = test_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "We have another `sort_and_gather` option which sorts the output according to GPU device ids and gather them together. \n",
    "\n",
    "For example, when a network B uses features extracted by a network A to make prediction and gather the outputs, it is important that the output of B is aligned to the label of A, so we probably need this option. But you can always do it by adding the label to `except_keywords` and adding the label to the output. And keep in mind it MUST be a cuda tensor, because we have to sort according to GPU ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
